{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A*PA evals\n",
    "\n",
    "This notebook contains the evals for A*PA.\n",
    "To reproduce figures from the paper, first unzip `results.zip` using `make unzip_results`.\n",
    "Alternatively, you can rerun all experiments (takes 10-20 hours) using `make run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "# ensures generated PDFs have a deterministic timestam on August 4th 2023.\n",
    "import os\n",
    "os.environ[\"SOURCE_DATE_EPOCH\"] = \"1691142813\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsize=10\n",
    "markersize=4\n",
    "linewidth = 0.75\n",
    "\n",
    "def column_display_name(col):\n",
    "    d = {\n",
    "        \"divergence\": \"Divergence\",\n",
    "        \"runtime\": \"Runtime per alignment [s]\",\n",
    "        \"runtime_capped\": \"Runtime per alignment [s]\",\n",
    "        \"s_per_pair\": \"Avg. runtime per alignment [s]\",\n",
    "        \"s_per_pair_capped\": \"Avg. runtime per alignment [s]\",\n",
    "        \"length\": \"Sequence length [bp]\",\n",
    "        \"band\": \"Equivalent band\",\n",
    "        \"algo_key\": \"algorithm\",\n",
    "        \"algo_pretty\": \" \",\n",
    "    }\n",
    "    if col in d:\n",
    "        return d[col]\n",
    "    return col\n",
    "\n",
    "dataset_pretty = {'ont-ul-500k': 'ONT reads', 'ont-minion-ul-500k': 'ONT reads + genetic variation'}\n",
    "dataset_order = list(dataset_pretty.keys())\n",
    "\n",
    "# Line style:\n",
    "# - slow (no pruning): dotted\n",
    "# - normal: solid\n",
    "# - diagonal-transition: dashed\n",
    "# Colours:\n",
    "# edlib/wfa ('extern'): blue/purple\n",
    "# sh/csh/gcsh: orange -> brown -> green gradient\n",
    "# noprune/normal/dt: 60% -> 70% -> 85% saturation\n",
    "colors = {'dijkstra': '#786061', 'sh': \"#e87146\", 'csh': \"#8c662a\", 'gcsh': \"#257d26\"}\n",
    "dashed = (0, (5, 5))\n",
    "dotted = (0, (1, 4))\n",
    "dashed_r1 = (0, (2.5, 5))\n",
    "algorithm_styles = {\n",
    "    \"edlib\": (\"#DE4AFF\", dashed, 'Edlib'),\n",
    "    \"biwfa\": (\"#625AFF\", '-', 'BiWFA'),\n",
    "    \"dijkstra\": (colors['dijkstra'], dashed, 'Dijkstra'),\n",
    "    \"gap\": ('#402020', dashed, 'A*-Gap'),\n",
    "    \"frequency\": ('#802020', dashed, 'A*-Freq'),\n",
    "    \"sh-noprune\": (colors['sh'], dotted, 'SH (no prune)'),\n",
    "    \"sh\": (colors['sh'], dashed, 'SH'),\n",
    "    \"csh\": (colors['csh'], dashed, 'CSH'),\n",
    "    \"gcsh\": (colors['gcsh'], dashed, 'GCSH'),\n",
    "    \"dijkstra-dt\": (colors['dijkstra'], '-', 'Dijkstra+DT'),\n",
    "    \"sh-dt\": ('#e35522', '-', 'SH+DT'),\n",
    "    \"csh-dt\": ('#875a12', '-', 'CSH+DT'),\n",
    "    \"gcsh-dt\": ('#0f7a10', '-', 'GCSH+DT'),\n",
    "    'astarpa': ('#0f7a10', '-', 'GCSH+DT'),\n",
    "    'unknown': ('#000000', '-', 'Unknown')\n",
    "}\n",
    "algorithm_order = list(algorithm_styles.keys())\n",
    "\n",
    "def get_algorithm_key(row):\n",
    "    name = row['algo_name']\n",
    "    if name == 'Edlib': return 'edlib'\n",
    "    if name == 'Wfa':\n",
    "        if row['job_algo_Wfa_memorymodel'] == 'MemoryUltraLow':\n",
    "            return 'biwfa'\n",
    "        else:\n",
    "            return 'wfa'\n",
    "    if name == 'AstarPa':\n",
    "        t = row['job_algo_AstarPa_heuristic_type']\n",
    "        dt = row['job_algo_AstarPa_diagonaltransition']\n",
    "        prune = row['job_algo_AstarPa_heuristic_prune'] if 'job_algo_AstarPa_heuristic_prune' in row else 'Both'\n",
    "        if t == 'None':\n",
    "            key = 'dijkstra'\n",
    "        else:\n",
    "            key = t.lower()\n",
    "        if t != 'None' and prune == 'None':\n",
    "            key += '-noprune'\n",
    "        if dt:\n",
    "            key += '-dt'\n",
    "        if key == 'gapcost': return 'unknown'\n",
    "        return key\n",
    "    return 'unknown'\n",
    "\n",
    "# Returns display color, style, and name for an algorithm\n",
    "def algorithm_display(row, split, default_r=None):\n",
    "    (c, l, n) = algorithm_styles[row['algo_key']]\n",
    "    if 'length' in split and 'algo_key' not in split:\n",
    "        n = f'$n=10^{int(math.log10(row.length))}$'\n",
    "        if 'r' in split and row.r == 1:\n",
    "            c = '#000000'\n",
    "            #l = dashed_r1\n",
    "    if 'r' in split:\n",
    "        if 'algo_key' not in split and row.r == 1:\n",
    "            c = '#000000'\n",
    "            pass\n",
    "        if row.r != 0 and (default_r is None or row.r != default_r):\n",
    "            n += f' ($r={row.r}$)'\n",
    "    \n",
    "    return (c, l, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(path):\n",
    "    # - Read a json file\n",
    "    # - Rename json fields from a_b to a-b\n",
    "    # - Flatten into dataframe\n",
    "    # - Flatten algorithm params into a few fields:\n",
    "    #   - algo_name: the type of algorithm\n",
    "    #   - algo_full: the json-string of algorithm parameters\n",
    "    # - Rename and compute some common columns:\n",
    "    #   - error-rate\n",
    "    #   - length\n",
    "    #   - s_per_pair\n",
    "    #   - p_correct\n",
    "    \n",
    "    json_path = Path(path)\n",
    "    data = json.loads(json_path.read_text())\n",
    "    \n",
    "    # Remove underscores from all keys\n",
    "    def remove_underscores(o):\n",
    "        if isinstance(o, list):\n",
    "            return [remove_underscores(v) for v in o]\n",
    "        if isinstance(o, dict):\n",
    "            return {k.replace('_', ''): remove_underscores(v) for k, v in o.items()}\n",
    "        return o\n",
    "    \n",
    "    data = remove_underscores(data)\n",
    "\n",
    "    # Clean up algo columns\n",
    "    for x in data:\n",
    "        name = list(x['job']['algo'].keys())[0]\n",
    "        obj = x['job']['algo']\n",
    "        obj['name'] = name\n",
    "        x['algo_name'] = name\n",
    "        x['algo_full'] = json.dumps(obj)\n",
    "        #del x['job']['algo']\n",
    "        if 'Ok' in x['output']:\n",
    "            del x['output']['Ok']['costs']\n",
    "\n",
    "    # Flatten the js\n",
    "    df = pd.json_normalize(data, sep='_')\n",
    "    df['algo_key'] = df.apply(get_algorithm_key, axis=1)\n",
    "    df['algo_pretty'] = df['algo_key'].map(lambda key: algorithm_styles[key][2])\n",
    "    \n",
    "    # Convenience renaming\n",
    "    df = df.rename({'job_dataset_Generated_length': 'length',\n",
    "                    'job_dataset_Generated_errorrate': 'errorrate',\n",
    "                    'job_timelimit': 'timelimit',\n",
    "                    'output_Ok_pcorrect': 'pcorrect',\n",
    "                    'output_Ok_measured_runtime': 'runtime',\n",
    "                    'output_Ok_measured_memory': 'memory',\n",
    "                    'stats_divergence_mean': 'divergence',\n",
    "                    'job_algo_AstarPa_diagonaltransition': 'dt',\n",
    "                    'job_algo_AstarPa_heuristic_prune': 'prune',\n",
    "                    'job_algo_AstarPa_heuristic_r': 'r',\n",
    "                    'job_algo_AstarPa_heuristic_k': 'k',\n",
    "                   }, axis='columns')\n",
    "    \n",
    "    # Order rows\n",
    "    df['algo_ord'] = df['algo_key'].map(lambda key: algorithm_order.index(key))\n",
    "    if 'k' in df.columns:\n",
    "        df.sort_values(by='k', inplace=True, kind = 'stable')\n",
    "    if 'r' in df.columns:\n",
    "        df.sort_values(by='r', inplace=True, kind = 'stable')\n",
    "    df.sort_values(by='algo_ord', inplace=True, kind = 'stable')\n",
    "    if 'length' not in df.columns and 'stats_length_mean' in df.columns:\n",
    "        df['length'] = df.stats_length_mean\n",
    "    if 'length' in df.columns:\n",
    "        df.sort_values(by='length', inplace=True, kind = 'stable')\n",
    "    if 'errorrate' in df.columns:\n",
    "        df.sort_values(by='errorrate', inplace=True, kind = 'stable')\n",
    "    # Order by dataset\n",
    "    if 'job_dataset_File' in df.columns and df.job_dataset_File.notna().all():\n",
    "        df['dataset'] = df['job_dataset_File'].map(lambda f: Path(f).parent.name)\n",
    "        df['dataset_ord'] = df['dataset'].map(lambda key: dataset_order.index(key))\n",
    "        df.sort_values(by='dataset_ord', inplace=True, kind = 'stable')\n",
    "    \n",
    "    # Computed columns\n",
    "    df['costmodel'] = df.apply(lambda row: (row['job_costs_sub'], row['job_costs_open'], row['job_costs_extend']), axis=1)\n",
    "    df['s_per_pair'] = df['runtime'] / df['stats_seqpairs']\n",
    "    df['timelimit_per_pair'] = df['timelimit'] / df['stats_seqpairs']\n",
    "    if 'length' in df.columns and 'output_Ok_stats_expanded' in df.columns:\n",
    "        df['band'] = df['output_Ok_stats_expanded'] / (df['stats_seqpairs']* df['length'])\n",
    "\n",
    "    def bad_k(row):\n",
    "        # Bad k only makes sense for A*PA with SH-based heuristic.\n",
    "        if row['algo_name'] != 'AstarPa':\n",
    "            return False\n",
    "        \n",
    "        h = row['job_algo_AstarPa_heuristic_type'].lower()\n",
    "        if h not in ['sh', 'csh', 'gcsh']:\n",
    "            return False\n",
    "        \n",
    "        # Too small k => too many matches\n",
    "        if row.k < math.log(row.length, 4) + (2 if row.r == 2 else 0):\n",
    "            return True\n",
    "        \n",
    "        # Too large k => not enough potential\n",
    "        if row.k > row.r/row.divergence:\n",
    "            return True\n",
    "        \n",
    "        # all is fine\n",
    "        return False\n",
    "\n",
    "    if 'length' in df.columns:\n",
    "        df['bad_k'] = df.apply(bad_k, axis=1)\n",
    "\n",
    "    def runtime_capped(row):\n",
    "        if not math.isnan(row['runtime']):\n",
    "            return row['runtime']\n",
    "        if row['output_Err'] == 'Timeout':\n",
    "            return row['timelimit']\n",
    "        return row['timelimit']*1.1\n",
    "    df['runtime_capped'] = df.apply(runtime_capped, axis = 1)\n",
    "    df['s_per_pair_capped'] = df['runtime_capped'] / df['stats_seqpairs']\n",
    "    \n",
    "    df['editdistance'] = df['stats_insertions'] + df['stats_deletions'] + df['stats_substitutions']\n",
    "    \n",
    "    # Some specific fixes\n",
    "    df = df.fillna({'r': 0}, downcast='infer')\n",
    "    \n",
    "    # Remove unsupported algos\n",
    "    if 'output_Err' in df.columns:\n",
    "        df = df[df.output_Err != 'Unsupported']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The one plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df,\n",
    "         name='',\n",
    "         file=None,\n",
    "         x='length',\n",
    "         y='s_per_pair',\n",
    "         # Column to use for hue and style.\n",
    "         # Always change both at the same time!\n",
    "         hue='algo_key',\n",
    "         style='r',\n",
    "         # column to use for marker size\n",
    "         size=None,\n",
    "         # Logarithmic axes by default\n",
    "         xlog=True,\n",
    "         ylog=True,\n",
    "         ylim=None,\n",
    "         xlim=None,\n",
    "         # alph\n",
    "         alpha=1.0,\n",
    "         # Use line instead of scatter plot?\n",
    "         connect=False,\n",
    "         # Draw a cone from the given filter and x\n",
    "         cone=None,\n",
    "         cone_x=3*10**4,\n",
    "         fit=False,\n",
    "         line_labels=False,\n",
    "         categorical=False,\n",
    "         ax=None,\n",
    "         width=None,\n",
    "         height=None,\n",
    "         default_r=None,\n",
    "         bad_k=True,\n",
    "        ):\n",
    "    \n",
    "    if df[y].isna().all():\n",
    "        print(f\"All values of {y} are nan.\")\n",
    "        return\n",
    "    \n",
    "    df = df[df[y].notnull()]\n",
    "    assert not df.empty\n",
    "    \n",
    "    # We group data by this set of keys.\n",
    "    split = [hue, style]\n",
    "    \n",
    "    # Remove 'r' from the split if not both r=1 and r=2 are present,\n",
    "    # to prevent redundant (r=1) in plots.\n",
    "    if 'r' in split and 'r' in df.columns:\n",
    "        if not (1 in df.r.values and 2 in df.r.values):\n",
    "            split.remove('r')\n",
    "    if 'r' in split and 'r' not in df.columns:\n",
    "        split.remove('r')\n",
    "    \n",
    "    # Group the data into datapoints per line\n",
    "    groups = df.groupby(split, sort=False)\n",
    "    \n",
    "    # Not sure if needed actually.\n",
    "    sns.reset_defaults()\n",
    "    sns.set_context(None) # 'paper', 'notebook'\n",
    "    \n",
    "    # Set up the figure if not provided.\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(width if width else 3, height if height else 2, forward=True)\n",
    "        hasax = False\n",
    "    else:\n",
    "        hasax = True\n",
    "\n",
    "    \n",
    "    # Set log scales\n",
    "    ax.set(xscale='log' if xlog else 'linear', yscale='log' if ylog else 'linear')\n",
    "    \n",
    "    # limit number of ticks\n",
    "    if ylog:\n",
    "        ax.locator_params(axis='y', numticks=6)\n",
    "    else:\n",
    "        ax.locator_params(axis='y', nbins=6)\n",
    "    \n",
    "    \n",
    "    # PLOTTING\n",
    "    \n",
    "    if not categorical:\n",
    "        # Show a scatterplot of points.\n",
    "        # Each group is plotted separately for more control over its style.\n",
    "        for k, group in groups:\n",
    "            first_row = group.iloc[0]\n",
    "            color, linestyle, grouplabel = algorithm_display(first_row, split, default_r)\n",
    "\n",
    "            ax.plot(x,\n",
    "                    y,\n",
    "                    data=group.sort_values(by=x),\n",
    "                    color=color,\n",
    "                    linestyle=linestyle if connect else 'None',\n",
    "                    marker='' if bad_k else 'o',\n",
    "                    alpha=alpha,\n",
    "                    dash_capstyle = 'round',\n",
    "                    label=grouplabel,\n",
    "                    zorder=2,\n",
    "                    markersize=markersize,\n",
    "                    linewidth=linewidth,\n",
    "                   )\n",
    "            \n",
    "            if bad_k:\n",
    "                # Draw separate scatter plots with red dots for bad k, and dots in the original color for good k.\n",
    "                ax.scatter(x,\n",
    "                           y,\n",
    "                           data=group[group.bad_k].sort_values(by=x),\n",
    "                           color='red',\n",
    "                           marker='o',\n",
    "                           alpha=alpha,\n",
    "                           zorder=2,\n",
    "                           s=markersize**2,\n",
    "                   )\n",
    "                ax.scatter(x,\n",
    "                           y,\n",
    "                           data=group[~group.bad_k].sort_values(by=x),\n",
    "                           color=color,\n",
    "                           marker='o',\n",
    "                           alpha=alpha,\n",
    "                           zorder=2,\n",
    "                           s=markersize**2,\n",
    "                   )\n",
    "                    \n",
    "    if categorical:\n",
    "        # Map algorithms to their color, unless bad_k is set, in which case those are made red.\n",
    "        hue_vec = df.apply(lambda row: 'red' if bad_k and row.bad_k else algorithm_display(row, split, default_r)[0], axis=1)\n",
    "        palette = {algorithm_display(row, split, default_r)[0]: algorithm_display(row, split, default_r)[0] for index, row in df.iterrows()} | {'red': 'red'}\n",
    "        # Overlay a boxplot and swarmplot on top of each other\n",
    "        sns.swarmplot(data=df,\n",
    "                        x=x,\n",
    "                        y=y,\n",
    "                        hue=hue_vec,\n",
    "                        palette=palette,\n",
    "                        ax=ax,\n",
    "                        size=3,\n",
    "                        linewidth=0,\n",
    "                        edgecolor='gray',\n",
    "                        zorder=10,\n",
    "                        dodge=False,\n",
    "        )\n",
    "        sns.boxplot(data=df,\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    zorder=9,\n",
    "                    ax=ax,\n",
    "                    boxprops={'facecolor':'None'},\n",
    "                    showfliers=False,\n",
    "                    linewidth=linewidth,\n",
    "                   )\n",
    "    \n",
    "    # TEXT\n",
    "    \n",
    "    # Title\n",
    "    if name:\n",
    "        ax.set_title(name, y=1.05)\n",
    "    \n",
    "    # Remove legend\n",
    "    ax.legend().remove()\n",
    "    \n",
    "    # BACKGROUND\n",
    "    ax.set_facecolor(\"#F8F8F8\")\n",
    "    ax.set_axisbelow(True) \n",
    "    ax.grid(False)\n",
    "    ax.grid(True, axis=\"y\", which=\"major\", color=\"white\", alpha=1, zorder=0)\n",
    "    \n",
    "    \n",
    "    # AXES\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(column_display_name(x))  # weight='bold',\n",
    "    ax.set_ylabel(column_display_name(y), rotation=0, ha=\"left\")\n",
    "    ax.yaxis.set_label_coords(-0.10, 1.00)\n",
    "    \n",
    "    # Limits\n",
    "    x_margin = 1.5\n",
    "    y_margin = 3\n",
    "    if xlog:\n",
    "        #xs = df[df[x] > 0][x]\n",
    "        ax.set_xlim(df[x].min() / x_margin, df[x].max() * x_margin)\n",
    "\n",
    "    if ylog:\n",
    "        ax.set_ylim(df[y].min() / y_margin, df[y].max() * y_margin)\n",
    "    \n",
    "    # Start linear scales at 0.\n",
    "    if not xlog and not categorical and x != 'job_costs_open':\n",
    "        ax.set(xlim=(0,None))\n",
    "    if not ylog:\n",
    "        ax.set(ylim=(0,None))\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim[0], ylim[1])\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim[0], xlim[1])\n",
    " \n",
    "    \n",
    "    # Show bottom spine, and left spine when xlog=false\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['left'].set_visible(not xlog and not categorical and (xlim is None or xlim[0] == 0))\n",
    "    \n",
    "    # Format % scales.\n",
    "    if x in ['errorrate', 'divergence']:\n",
    "        ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "    if x == 'k' and not categorical:\n",
    "        ax.xaxis.set_major_locator(mtick.MultipleLocator(base=2,offset=1))\n",
    "    \n",
    "    # Show major ticks\n",
    "    ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"major\",\n",
    "        bottom=True,\n",
    "        top=False,\n",
    "        left=True,\n",
    "        right=False,\n",
    "    )\n",
    "    # No minor ticks\n",
    "    ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"minor\",\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False,  # labels along the bottom edge are off\n",
    "    )\n",
    "    # Do show minor ticks for small log ranges\n",
    "    #if ylog:\n",
    "    #     ax.tick_params(axis=\"y\", which=\"minor\", left=True)\n",
    "    \n",
    "    \n",
    "    # CONE\n",
    "    # Fills the region between x**1 and x**2\n",
    "    if cone:\n",
    "        x0 = cone_x\n",
    "        x_max = x_margin * df[x].max()\n",
    "        x_range = (x0, x_max)\n",
    "        \n",
    "        y0 = df[cone(df) & (df[x] == cone_x)][y].max()\n",
    "        y_lin = (y0, y0 * (x_max / x0) ** 1)\n",
    "        y_quad = (y0, y0 * (x_max / x0) ** 2)\n",
    "        ax.fill_between(x_range, y_lin, y_quad, color=\"grey\", alpha=0.15, zorder=0.4)\n",
    "        \n",
    "    # TIME LIMIT\n",
    "    if y=='runtime_capped' or (y=='s_per_pair_capped' and x != 'length'):\n",
    "        timelimit = df.timelimit.iloc[0]\n",
    "        #assert df[df.runtime.isna()].timelimit.eq(timelimit).all()\n",
    "        # Draw a red line at the timelimit.\n",
    "        ax.axhline(y=timelimit, color=\"red\", linestyle=\"-\", alpha=1, linewidth=0.5)\n",
    "        \n",
    "        # Modify/add the timelimit ticklabel with TL=\n",
    "        if False:\n",
    "            ylabels = [x for x in ax.get_yticklabels()]\n",
    "            found = False\n",
    "            for i, l in enumerate(ylabels):\n",
    "                if l.get_position()[1] == timelimit:\n",
    "                    ylabels[i] = \"TL=\" + ylabels[i].get_text()\n",
    "                    found = True\n",
    "            if found:\n",
    "                ax.set_yticklabels(ylabels)\n",
    "            else:\n",
    "                yticks = list(ax.get_yticks())\n",
    "                ylabels = list(ax.get_yticklabels())\n",
    "                yticks.append(timelimit)\n",
    "                ylabels.append(\"TLE\")\n",
    "                ax.set_yticks(yticks)\n",
    "                try:\n",
    "                    ax.set_yticklabels(ylabels)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                finally:\n",
    "                    pass\n",
    "            \n",
    "    # POLY FIT\n",
    "\n",
    "    def angle(slope):\n",
    "        x_min, x_max = ax.get_xlim()\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        bbox = ax.get_window_extent()\n",
    "        x_sz = bbox.width\n",
    "        y_sz = bbox.height\n",
    "        x_factor = x_sz / (np.log10(x_max) - np.log10(x_min) if xlog else x_max - x_min)\n",
    "        y_factor = y_sz / (np.log10(y_max) - np.log10(y_min) if ylog else y_max - y_min) \n",
    "        slope = slope * y_factor / x_factor\n",
    "        return math.atan(slope)*180/math.pi\n",
    "    \n",
    "    if fit:\n",
    "        assert x=='length' and xlog and ylog, \"Polynomial fits only work in log-log plots with x=length\"\n",
    "        for k, group in groups:\n",
    "            first_row = group.iloc[0]\n",
    "            color, linestyle, grouplabel = algorithm_display(first_row, split, default_r)\n",
    "            fit_label = grouplabel\n",
    "            \n",
    "            filtered = group[group.runtime.notnull()]\n",
    "            ps = filtered[[x,y]].dropna()\n",
    "            xmin, xmax = filtered[x].min(), filtered[x].max()\n",
    "            if len(ps) > 1:\n",
    "                fit = np.polyfit(np.log(ps[x]), np.log(ps[y]), 1)\n",
    "                f = lambda x: x**fit[0] * np.exp(fit[1])\n",
    "                # Extra {{ and }} are for the math-mode superscript\n",
    "                fit_label = f\"{grouplabel} $\\sim n^{{{fit[0]:0.2f}}}$\"\n",
    "\n",
    "                ymin, ymax = f(xmin), f(xmax)\n",
    "                # line from xmin to xmax (use plt.axline for infinite line)\n",
    "                ax.plot([xmin, xmax], [ymin, ymax], color=color, linestyle=linestyle, alpha=1, dash_capstyle = 'round', label=grouplabel, zorder=2, linewidth=linewidth)\n",
    "                #print(f'Exponent for {k}: {fit[0]:0.2f}')\n",
    "            else:\n",
    "                ymax = float('inf')\n",
    "                fit = [0]\n",
    "                \n",
    "\n",
    "            ax.text(\n",
    "                xmax,\n",
    "                min(ymax, ax.get_ylim()[1]),\n",
    "                fit_label,\n",
    "                color=color,\n",
    "                ha=\"right\",\n",
    "                va=\"bottom\",\n",
    "                size=labelsize,\n",
    "                alpha=1,\n",
    "                rotation=angle(fit[0]),\n",
    "                rotation_mode='anchor',\n",
    "            )\n",
    "    if line_labels:\n",
    "        # If no legend and no fits are shown, show manual labels instead\n",
    "        for split_key, group in groups:\n",
    "            first_row = group.iloc[0]\n",
    "            color, linestyle, grouplabel = algorithm_display(first_row, split, default_r)\n",
    "            max_idx = group[x].idxmax()\n",
    "            label_x = group[x][max_idx]\n",
    "            label_y = min(group[y][max_idx], ax.get_ylim()[1])\n",
    "            key = split_key[0] if isinstance(split_key, tuple) else split_key\n",
    "            \n",
    "            by_x = group[x].argsort()\n",
    "            last = group.iloc[by_x.iloc[-1]]\n",
    "            try:\n",
    "                before = group.iloc[by_x.iloc[-3]]\n",
    "                slope = (last[y] - before[y])/(last[x] - before[x])\n",
    "            except:\n",
    "                slope = 0\n",
    "            ax.text(\n",
    "                label_x,\n",
    "                label_y,\n",
    "                grouplabel,\n",
    "                color=color,\n",
    "                ha=\"right\",\n",
    "                va=\"bottom\",\n",
    "                size=labelsize,\n",
    "                alpha=1,\n",
    "                rotation=angle(slope),\n",
    "                rotation_mode='anchor',\n",
    "            )\n",
    "\n",
    "    if not hasax:\n",
    "        if file:\n",
    "            plt.savefig(f\"plots/{file}.pdf\", dpi=300, bbox_inches='tight')\n",
    "            #plt.savefig(f\"plots/{file}.png\", dpi=300, bbox_inches='tight')\n",
    "            \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check: CPU frequency = 2600MHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"results/cache.json\")\n",
    "df = df.rename({'output_Ok_measured_cpufreqstart': 'freqstart','output_Ok_measured_cpufreqend': 'freqend'}, axis='columns')\n",
    "for c in ['freqstart', 'freqend']:\n",
    "    print(df[c].min(), df[c].max())\n",
    "    assert df[c].min() > 2550\n",
    "    assert df[c].max() < 2650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error rate to divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error rate to divergence mapping')\n",
    "df = read_results(\"results/memory.json\")\n",
    "display(df[df.length==1000000].pivot_table(index='errorrate', values = 'divergence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper figures and tables\n",
    "## Scaling with length (Fig 4ab, appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_results(\"results/scaling_n.json\")\n",
    "df.loc[df['algo_key'] == 'edlib', 'dt'] = False\n",
    "df.loc[df['algo_key'] == 'biwfa', 'dt'] = True\n",
    "\n",
    "\n",
    "cone = lambda df: (df['algo_key'] == 'sh') & (df['r'] == (1 if e <= 0.05 else 2))\n",
    "for e, g in df.groupby('errorrate'):\n",
    "    default_r = None\n",
    "    if e == 0.05:\n",
    "        g = g[g.algo_key != 'csh']\n",
    "        g = g[g.algo_key != 'csh-dt']\n",
    "        # at r=2, only SH\n",
    "        g = g[(g.r != 2) | (g.algo_key == 'sh')]\n",
    "        # for GCSH, only with DT\n",
    "        g = g[g.algo_key != 'gcsh' ]\n",
    "        default_r = 1\n",
    "    if e == 0.15:\n",
    "        g = g[g.algo_key != 'csh']\n",
    "        g = g[g.algo_key != 'csh-dt']\n",
    "        # at r=1, only GCSH\n",
    "        g = g[(g.r != 1) | (g.algo_key == 'gcsh') | (g.algo_key == 'gcsh-dt')]\n",
    "        default_r = 2\n",
    "    plot(g, file=f'scaling_n_e{e}', x='length', y='s_per_pair', fit=True, cone=cone, cone_x = 3000, width=4.4, height=3, ylim = (10**-3.9, 10**3.9), default_r=default_r)\n",
    "plt.show()\n",
    "\n",
    "assert(df[df.output_Err.notna()].output_Err.apply(lambda x: x in ['Skipped', 'MemoryLimit']).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Near-linear scaling and 500x speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scaling of A*PA\n",
    "df = read_results(\"results/scaling_n.json\")\n",
    "s = df.pivot(index=['errorrate','algo_key', 'r'], values='s_per_pair', columns='length')\n",
    "fits = s.apply(lambda row: np.polyfit(np.log(row.index), np.log(row), 1)[0], axis=1).round(2)\n",
    "#display(fits)\n",
    "display(fits.loc[([0.05, 0.15], ['sh', 'gcsh-dt'])])\n",
    "print('Best fit for SH at 5% for r=1 is ', fits[0.05]['sh'][1], ' and GCSH-DT at 15% for r=2 is ', fits[0.15]['gcsh-dt'][2])\n",
    "\n",
    "# Compute max speedup over edlib and biwfa for e=5%, r=1\n",
    "df = df[df.errorrate == 0.05]\n",
    "df = df[df.r != 2]\n",
    "s = df[df.length == 10**7].pivot(index='algo_key', values='s_per_pair', columns='errorrate')\n",
    "#print('Time for n=10^7, e=5%, r=1')\n",
    "#display(s)\n",
    "s /= s.loc['sh-dt']\n",
    "s = s.round(1)\n",
    "print('Speedup of SH+DT')\n",
    "display(s)\n",
    "speedup=500\n",
    "print('Speedup of SH+DT over edlib at 5%, 10^7 is ', s[0.05]['edlib'], f' > {speedup}')\n",
    "print('Speedup of SH+DT over biwfa at 5%, 10^7 is ', s[0.05]['biwfa'], f' > {speedup}')\n",
    "assert s[0.05]['edlib'] > speedup\n",
    "assert s[0.05]['biwfa'] > speedup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix C: larger figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"results/scaling_n.json\")\n",
    "#df = df[df.algo_key.isin(['edlib', 'biwfa', 'sh', 'gcsh-dt'])]\n",
    "\n",
    "for (e, g) in df.groupby('errorrate'):\n",
    "    cone = lambda df: (df['algo_key'] == 'sh') & (df['r'] == (1 if e <= 0.05 else 2))\n",
    "    file = f'scaling_n_e{e:.2f}_large'\n",
    "    plot(g, file=f'scaling_n_e{e}_large', x='length', y='s_per_pair', fit=True, cone=cone, cone_x = 3000, width=1.57*4.4, height=2.2*3, ylim = (10**-3.9, 10**3.9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix C: Equivalent band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_results(\"results/scaling_n.json\")\n",
    "df = df[~df.algo_key.isin(['edlib', 'biwfa', 'dijkstra', 'dijkstra-dt', 'sh-noprune', 'gap', 'frequency'])]\n",
    "\n",
    "for ((e, r), g) in df.groupby(['errorrate', 'r']):\n",
    "    if e==0.15 and r==1: continue\n",
    "    if e == 0.05:\n",
    "        ylim = (1, 2.1)\n",
    "    else:\n",
    "        ylim = (1, 19)\n",
    "    plot(g, file=f'band_e{e:.2f}_r{r}', x='length', y='band', connect=True, line_labels=True,\n",
    "                 ylog=False, ylim=ylim,  width=4.4, height=3,\n",
    "                )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling with divergence (Fig 4c, Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_results(\"results/scaling_e.json\")\n",
    "df = df[df.algo_key != 'unknown']\n",
    "display(df[df.output_Err.notna()].output_Err.unique())\n",
    "df = df[df.s_per_pair < 100]\n",
    "plot(df, file=f'scaling_e_zoom', x='divergence', y='s_per_pair', size=None, xlog=False, ylog=False, connect=True, line_labels=True,\n",
    "             ylim = (0, 0.4),\n",
    "             width=4.4, height=3)\n",
    "plot(df, file=f'scaling_e', x='divergence', y='s_per_pair', size=None, xlog=False, ylog=False, connect=True, line_labels=True,\n",
    "             ylim = (0, 99),\n",
    "             width=1.3*4.4, height=1.3*3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of divergence within each dataset\n",
    "Each datapoint in the plots above consists of 10 sequence pairs. The reported divergence is the average of this set. Just below each threshold, there will be some sequences in the set that have divergence above the threshold. This skews the thresholds as shown slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_results(\"results/scaling_e.json\")\n",
    "print('Max divergence per dataset')\n",
    "div = df[df.divergence > 0.09].pivot_table(index='divergence', values='stats_divergence_max')\n",
    "display(div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human data (Fig 5, Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df = read_results(\"results/real.json\")\n",
    "def boxplot(df, file, w):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(w, 3))\n",
    "    for (k, g), ax in zip(df.groupby('dataset', sort=False), axs):\n",
    "        print(k)\n",
    "        if file == 'real_astarpa':\n",
    "            g.loc[g.algo_key == 'gcsh-dt', 'algo_pretty'] = 'A*PA'\n",
    "        plot(g, x='algo_pretty', y='runtime_capped', xlog=False, ylog=True, categorical=True, ylim=(0.3, 150), ax=ax)\n",
    "\n",
    "        ax.set_xlabel(dataset_pretty[k])\n",
    "        if k == 'ont-minion-ul-500k' and file in ['real_astarpa']:\n",
    "            ax.set_xlabel('ONT reads + genetic var.')\n",
    "      \n",
    "    if file == 'real_astarpa':\n",
    "        axs[1].set_yticklabels([])\n",
    "        axs[1].set_ylabel(None)\n",
    "    fig.subplots_adjust(wspace=.15)\n",
    "    plt.savefig(f\"plots/{file}.pdf\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "#boxplot(df[df.algo_key.isin(['edlib', 'biwfa', 'gcsh-dt'])] , 'real_astarpa', 4.5)\n",
    "boxplot(df, 'real_full', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergence threshold 10%: All with d<10% pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"results/real.json\")\n",
    "df = df[df.algo_key == 'gcsh-dt']\n",
    "for key, g in df.groupby('dataset'):\n",
    "    print('dataset', key)\n",
    "    print('Num failures ', len(g[g.output_Err.notna()]))\n",
    "    min_fail = g[g.output_Err.notna()].divergence.min()\n",
    "    max_pass = g[g.output_Err.isna()].divergence.max()\n",
    "    print('Min failed divergence', min_fail)\n",
    "    print('Max passed divergence', max_pass)\n",
    "    print()\n",
    "    assert min_fail > 0.01, \"Seqs below 10% divergence should pass\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median speedup >3x / >1.7x on human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"results/real.json\")\n",
    "df = df[df.algo_key.isin(['edlib', 'biwfa', 'gcsh-dt'])]\n",
    "m = df.pivot_table(index='algo_key', columns=['dataset'], values='runtime_capped', aggfunc = np.median, sort=False)\n",
    "#display(m)\n",
    "m /= m.loc['gcsh-dt']\n",
    "print('speedup of gcsh-dt over others')\n",
    "display(m)\n",
    "assert m['ont-ul-500k']['edlib'] >= 3 and m['ont-ul-500k']['biwfa'] > 3\n",
    "assert m['ont-minion-ul-500k']['edlib'] >= 1.7 and m['ont-minion-ul-500k']['biwfa'] > 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A*PA Parameters (Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_results(\"results/params.json\")\n",
    "df['rn'] = df.apply(lambda row: f\"{row['r']}-{row['length']}\", axis=1)\n",
    "for d, g in df.groupby('errorrate'):\n",
    "    plot(g, file=f'params_e{d}', x='k', y='s_per_pair', xlog=False, ylog=True, connect=True, line_labels=True,\n",
    "             hue='length', style='r', xlim=(4, 26),\n",
    "             width=1.3*4.4, height=1.3*3)\n",
    "plt.show()\n",
    "\n",
    "display(df[df.output_Err.notna()].output_Err.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df = read_results(\"results/params_real.json\")\n",
    "def boxplot(df, file, w):\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(w, 3))\n",
    "    subfigs = fig.subfigures(1, 2)\n",
    "    for (k, g), subfig in zip(df.groupby(['dataset'], sort=False), subfigs):\n",
    "        subfig.suptitle(dataset_pretty[k], y=0)\n",
    "        axs = subfig.subplots(1, 2, sharey=True)\n",
    "        for (r, g), ax in zip(g.groupby(['r'], sort=False), axs): \n",
    "            plot(g, x='k', y='runtime_capped', hue='r', xlog=False, ylog=True, categorical=True, ylim=(0.3, 150), ax=ax)\n",
    "            ax.set_xlabel(f'$k$, {\"in\" if r==2 else \"\"}exact matches ($r={r}$)')\n",
    "\n",
    "    plt.savefig(f\"plots/{file}.pdf\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "boxplot(df, 'params_real', 12)\n",
    "\n",
    "display(df[df.output_Err.notna()].output_Err.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory usage (Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"results/memory.json\")\n",
    "df.memory = (df.memory/1000000).round(0).astype('int')\n",
    "df.sort_values(by='errorrate', inplace=True, kind='stable')\n",
    "table = df.pivot_table(index='algo_pretty', columns = 'errorrate', values = 'memory', sort=False)\n",
    "display(table)\n",
    "#print(table.to_latex())\n",
    "assert table.max().max() < 500, \"All should use at most 500MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human data memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_results(\"results/real.json\")\n",
    "df.memory = (df.memory/1000000)\n",
    "df['capped_memory'] = df.memory.fillna(1000000)\n",
    "df = df[df.algo_key.isin(['edlib', 'biwfa', 'gcsh-dt'])]\n",
    "table = df.pivot_table(index='algo_pretty', columns=['dataset'], values=['capped_memory', 'memory'], aggfunc={'capped_memory': np.median, 'memory': np.max}, sort=False).round(0).astype('int')\n",
    "table =table.rename({'capped_memory': 'Median', 'memory': 'Max'}, axis='columns')\n",
    "table = table.swaplevel(axis=1)\n",
    "table.sort_index(axis=1, level=0, inplace=True, kind='stable', ascending=False)\n",
    "display(table)\n",
    "#print(table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime profile (Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_results(\"results/timing.json\")\n",
    "time_labels = {'precomp': 'Precomputation', 'astar': 'A*: opening & expanding', 'h': 'A*: h() evaluation', 'pruning': 'A*: Pruning matches', 'contours': 'A*: Updating contours', 'reordering': 'A*: Reordering states', 'traceback': 'Traceback'}\n",
    "\n",
    "for c in df.columns:\n",
    "    prefix = 'output_Ok_stats_t'\n",
    "    if c.startswith(prefix):\n",
    "        name = c[len(prefix):]\n",
    "        label = time_labels.get(name, name)\n",
    "        df = df.rename({c: label}, axis='columns')\n",
    "\n",
    "# todo: stats_expanding\n",
    "df = df.pivot_table(index=['divergence', 'r'], values=time_labels.values(), sort=False)\n",
    "unique_errorrates = df.index.get_level_values('divergence').unique()\n",
    "unique_r_values = df.index.get_level_values('r').unique()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(unique_r_values), ncols=len(unique_errorrates), figsize=(6, 4))\n",
    "\n",
    "# Create a dictionary to store handles and labels for the legend\n",
    "handles_dict = {}\n",
    "size_factor = 0.065\n",
    "\n",
    "for r_idx, r_val in enumerate(unique_r_values):\n",
    "    for err_idx, err_val in enumerate(unique_errorrates):\n",
    "        ax = axes[r_idx, err_idx]\n",
    "        if (err_val, r_val) in df.index:\n",
    "            temp_df = df.loc[(err_val, r_val)]\n",
    "            total = temp_df.sum()\n",
    "            radius = np.sqrt(total * size_factor)\n",
    "            wedges, texts, autotexts = ax.pie(temp_df, autopct=\"\", pctdistance=0.8, radius=radius, startangle = 90, colors = sns.color_palette())\n",
    "            for wedge, label in zip(wedges, temp_df.index):\n",
    "                handles_dict[label] = wedge\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "        if r_idx == 1:\n",
    "            ax.set_xlabel(f'd={100*err_val:.0f}%', labelpad=0)\n",
    "        if err_idx == 0:\n",
    "            ax.set_ylabel(f'r={r_val}', labelpad=-15, rotation=0)\n",
    "\n",
    "# Add a single legend for all pie charts\n",
    "fig.legend(handles_dict.values(), handles_dict.keys(), loc='upper right')\n",
    "plt.tight_layout(pad=0, w_pad=-2.5)\n",
    "plt.subplots_adjust(hspace=-0.6)\n",
    "plt.savefig(f\"plots/timing_synthetic.pdf\", dpi=300, bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_results(\"results/timing_real.json\")\n",
    "df = df[df.output_Ok_stats_tastar.notna()]\n",
    "\n",
    "for c in df.columns:\n",
    "    prefix = 'output_Ok_stats_t'\n",
    "    if c.startswith(prefix):\n",
    "        name = c[len(prefix):]\n",
    "        label = time_labels.get(name, name)\n",
    "        df = df.rename({c: label}, axis='columns')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "\n",
    "for k, g in df.groupby('dataset'):\n",
    "    ax = axes[0 if k=='ont-ul-500k' else 1]\n",
    "    df = g\n",
    "    df.sort_values(by='runtime', inplace=True, kind = 'stable')\n",
    "    df.plot.bar(y = time_labels.values(), stacked=True, width=.9, zorder=2, ax=ax, color = sns.color_palette())\n",
    "    ax.set_ylabel('Runtime per alignment [s]', rotation=0, ha=\"left\")\n",
    "    ax.set_ylim(0, 11)\n",
    "    ax.set_xlabel(dataset_pretty[k])\n",
    "    if k == 'ont-minion-ul-500k':\n",
    "        ax.legend().remove()\n",
    "    ax.tick_params(\n",
    "        axis=\"x\",\n",
    "        which=\"both\",\n",
    "        bottom=False,\n",
    "    )\n",
    "    ax.set_xticklabels([])\n",
    "    ax.locator_params(axis='y', nbins=4)\n",
    "    ax.set_facecolor(\"#F8F8F8\")\n",
    "    ax.grid(False)\n",
    "    ax.grid(True, axis=\"y\", which=\"major\", color=\"w\")\n",
    "    ax.yaxis.set_label_coords(-0.1, 1.00)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "plt.savefig(f\"plots/timing_real.pdf\", dpi=300, bbox_inches='tight')    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "name": "evals.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
